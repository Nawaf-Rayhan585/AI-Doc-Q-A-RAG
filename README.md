# ğŸ“˜ AI-Doc â€” Intelligent Document Q&A & Summarizer (RAG-Based)

AI-Doc is a **Retrieval-Augmented Generation (RAG)** powered document assistant that allows you to **upload PDF or DOCX files**, ask natural language questions about them, and generate **AI-powered answers and summaries** using local LLMs.

The system combines **semantic search (FAISS + sentence embeddings)** with a **local Ollama LLM** to deliver fast, private, and context-aware responses â€” no cloud APIs required.

---

## ğŸš€ Features

* ğŸ“„ Upload & Parse **PDF and DOCX** documents
* ğŸ§© Automatic text chunking for large documents
* ğŸ§  Semantic search using **Sentence Transformers**
* âš¡ FAISS-powered vector similarity retrieval
* ğŸ¤– Context-aware Q&A using **Ollama (LLaMA-based models)**
* ğŸ§¾ One-click **AI Document Summarization**
* ğŸ’» Auto-detects **CPU / GPU** for embeddings
* ğŸ” Fully **local & private** AI pipeline
* ğŸ›ï¸ Clean and interactive **Streamlit UI**

---

## ğŸ–¥ï¸ UI Preview

### ğŸ“„ Document Upload Interface

<img width="1919" height="377" alt="image" src="https://github.com/user-attachments/assets/0f4cff2d-9618-43e8-b5b8-846b34492536" />

<br/>

### ğŸ’¬ Ask Questions About the Document

<img width="1919" height="896" alt="image" src="https://github.com/user-attachments/assets/15a56bd7-385b-451c-b25e-f1fa37171667" />


<br/>

### ğŸ§¾ AI-Generated Document Summary

<img width="419" height="891" alt="image" src="https://github.com/user-attachments/assets/58522f60-4a96-40ee-aa24-0d150be81449" />


<br/>

---

## ğŸ§  How It Works (High-Level)

1. ğŸ“¥ User uploads a PDF or DOCX file
2. ğŸ“„ Text is extracted and split into chunks
3. ğŸ§© Chunks are converted into embeddings
4. ğŸ“¦ FAISS stores vectors for fast retrieval
5. ğŸ” User query retrieves top relevant chunks
6. ğŸ¤– Ollama LLM generates an answer using context
7. ğŸ§¾ Optional full-document summarization

---

## âš™ï¸ How to Run the Project

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/ai-doc.git
cd ai-doc
```

### 2ï¸âƒ£ Create & Activate Virtual Environment (Recommended)

```bash
python -m venv venv
venv\Scripts\activate
```

> **Note:** On macOS/Linux, use `source venv/bin/activate`

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Install & Setup Ollama

* Download and install **Ollama**
* Pull a supported model (example):

```bash
ollama pull llama3.2:1b
```

> You can change the model name directly in `main.py`

### 5ï¸âƒ£ Run the Application

```bash
streamlit run main.py
```

### 6ï¸âƒ£ Open in Browser

The app will be available at:

```
http://localhost:8501
```

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **Streamlit** â€” UI
* **Sentence-Transformers** â€” Embeddings
* **FAISS** â€” Vector Search
* **Ollama** â€” Local LLM Inference
* **PyMuPDF (fitz)** â€” PDF Parsing
* **python-docx** â€” DOCX Parsing
* **Torch** â€” Device acceleration

---

## ğŸ“Œ Notes

* Designed for **local AI workflows**
* No external API keys required
* Best suited for **private documents**
* Scales well for long documents using RAG

---

## ğŸ“œ License

This project is open-source and available under the **MIT License**.

---

ğŸ”¥ Built with focus on **privacy, speed, and practical AI**
